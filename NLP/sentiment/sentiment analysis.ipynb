{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9483a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # Download latest version (this goes into KaggleHub cache folder)\n",
    "# path = kagglehub.dataset_download(\"kazanova/sentiment140\")\n",
    "\n",
    "# print(\"KaggleHub download path:\", path)\n",
    "\n",
    "# # Your custom target folder\n",
    "# custom_path = r\"E:\\github\\data science\\data-science\\NLP\"\n",
    "\n",
    "# # Make sure the folder exists\n",
    "# os.makedirs(custom_path, exist_ok=True)\n",
    "\n",
    "# # Copy everything from KaggleHub cache folder to your custom folder\n",
    "# for item in os.listdir(path):\n",
    "#     s = os.path.join(path, item)\n",
    "#     d = os.path.join(custom_path, item)\n",
    "#     if os.path.isdir(s):\n",
    "#         shutil.copytree(s, d, dirs_exist_ok=True)\n",
    "#     else:\n",
    "#         shutil.copy2(s, d)\n",
    "\n",
    "# print(\"Dataset copied to:\", custom_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc778c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "388fc27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   target         ids                          date      flag  \\\n",
      "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
      "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
      "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
      "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "\n",
      "              user                                               text  \n",
      "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
      "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
      "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
      "4           Karoli  @nationwideclass no, it's not behaving at all....  \n",
      "             target           ids\n",
      "count  1.600000e+06  1.600000e+06\n",
      "mean   2.000000e+00  1.998818e+09\n",
      "std    2.000001e+00  1.935761e+08\n",
      "min    0.000000e+00  1.467810e+09\n",
      "25%    0.000000e+00  1.956916e+09\n",
      "50%    2.000000e+00  2.002102e+09\n",
      "75%    4.000000e+00  2.177059e+09\n",
      "max    4.000000e+00  2.329206e+09\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   target  1600000 non-null  int64 \n",
      " 1   ids     1600000 non-null  int64 \n",
      " 2   date    1600000 non-null  object\n",
      " 3   flag    1600000 non-null  object\n",
      " 4   user    1600000 non-null  object\n",
      " 5   text    1600000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(r\"E:\\github\\data science\\data-science\\NLP\\training.1600000.processed.noemoticon.csv\",encoding='latin-1',header=None)\n",
    "df.columns=['target','ids','date','flag','user','text']\n",
    "df.fillna('')\n",
    "print(df.head())\n",
    "print(df.describe())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4752c69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"should've\", 'should', 'can', 'above', 'again', \"we've\", 'into', 'ma', \"haven't\", 'off', 'we', 'hasn', 'yourself', 'himself', 'm', 'other', 'which', 'a', 'yourselves', 'were', \"mustn't\", 'am', 'an', 'don', 'after', \"it'd\", 'this', 'how', 'only', 'between', 'been', 'me', 'such', \"you'd\", 'he', \"shan't\", 'its', 'about', 'had', 'couldn', 'because', 'having', 're', 'as', 'own', 'ourselves', 'it', \"we'll\", 'that', 'when', 't', \"don't\", 'the', \"he'll\", 'here', 'most', 'needn', 'our', 'herself', \"she'll\", 'your', 'hadn', \"i've\", 'not', \"she'd\", \"she's\", \"needn't\", \"he's\", 'him', 'each', 'shouldn', 'y', \"i'm\", 'whom', 'in', \"you'll\", \"aren't\", 'they', 'during', \"they'll\", \"didn't\", 'his', 'further', 'wasn', 'why', 'my', 'at', 'if', 'll', \"doesn't\", 'until', 'aren', 'do', 'once', \"isn't\", 'hers', \"that'll\", \"it'll\", 'didn', 'isn', 'was', \"couldn't\", 'for', 've', \"wasn't\", 'haven', 'doing', 'out', 'before', 'there', 'while', 'their', 'with', 'what', 'to', 'themselves', \"wouldn't\", 'no', \"i'd\", 'down', 'are', 'these', \"they'd\", \"we're\", 'very', 'and', 'of', 'will', 's', \"it's\", \"you're\", 'through', 'itself', 'her', 'same', 'up', 'more', 'some', \"we'd\", 'being', 'any', 'those', 'you', 'is', 'doesn', 'or', 'has', 'she', 'too', 'all', 'nor', 'from', \"they've\", 'by', \"he'd\", 'over', 'who', \"weren't\", 'where', 'wouldn', \"you've\", 'yours', 'than', \"i'll\", 'd', 'does', 'against', 'theirs', 'won', \"shouldn't\", \"won't\", 'on', 'be', 'i', 'both', 'have', \"they're\", 'few', 'so', 'mustn', 'o', 'then', 'weren', 'ain', 'ours', 'under', 'but', 'below', 'them', 'myself', 'mightn', \"mightn't\", \"hadn't\", 'did', 'now', 'shan', \"hasn't\", 'just'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26146aba",
   "metadata": {},
   "source": [
    "0->negative\n",
    "1->positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "513d5c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example:sampl text test stem token function\n"
     ]
    }
   ],
   "source": [
    "port_stem = PorterStemmer()\n",
    "def stemming_tokenizer(text):\n",
    "    stem_content=re.sub('[^a-zA-Z]',' ',text)\n",
    "    stem_content=stem_content.lower()\n",
    "    stem_content=stem_content.split()\n",
    "    stem_content=[port_stem.stem(word) for word in stem_content if not word in stop_words]\n",
    "    stem_content=' '.join(stem_content)\n",
    "    return stem_content\n",
    "print(f'example:{stemming_tokenizer(\"This is a sample text. We\\'re testing the stemming_tokenizer function!\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d5c21b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stemming only to the first 1000 rows\n",
    "df.loc[:99999, 'stemmed_text'] = df.loc[:99999, 'text'].apply(stemming_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1de7a9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   target         ids                          date      flag  \\\n",
      "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
      "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
      "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
      "\n",
      "              user                                               text  \\\n",
      "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
      "1    scotthamilton  is upset that he can't update his Facebook by ...   \n",
      "2         mattycus  @Kenichan I dived many times for the ball. Man...   \n",
      "\n",
      "                                        stemmed_text  \n",
      "0  switchfoot http twitpic com zl awww bummer sho...  \n",
      "1  upset updat facebook text might cri result sch...  \n",
      "2  kenichan dive mani time ball manag save rest g...  \n",
      "0          switchfoot http twitpic com zl awww bummer sho...\n",
      "1          upset updat facebook text might cri result sch...\n",
      "2          kenichan dive mani time ball manag save rest g...\n",
      "3                            whole bodi feel itchi like fire\n",
      "4                              nationwideclass behav mad see\n",
      "                                 ...                        \n",
      "1599995                                                     \n",
      "1599996                                                     \n",
      "1599997                                                     \n",
      "1599998                                                     \n",
      "1599999                                                     \n",
      "Name: stemmed_text, Length: 1600000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['stemmed_text'] = df['stemmed_text'].fillna(\"\")\n",
    "print(df.head(3))\n",
    "print(df['stemmed_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23cc2525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0\n",
       "1          0\n",
       "2          0\n",
       "3          0\n",
       "4          0\n",
       "          ..\n",
       "1599995    4\n",
       "1599996    4\n",
       "1599997    4\n",
       "1599998    4\n",
       "1599999    4\n",
       "Name: target, Length: 1600000, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a056432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['switchfoot http twitpic com zl awww bummer shoulda got david carr third day'\n",
      " 'upset updat facebook text might cri result school today also blah'\n",
      " 'kenichan dive mani time ball manag save rest go bound' ... '' '' '']\n"
     ]
    }
   ],
   "source": [
    "x=df['stemmed_text'].values\n",
    "y=df['target'].values\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69ee0fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280000, 54740) (320000, 54740)\n",
      "  (19, 12780)\t0.7176092382913959\n",
      "  (19, 36214)\t0.6964459642490884\n",
      "  (28, 2284)\t0.5415354953525429\n",
      "  (28, 42873)\t0.5367268420191788\n",
      "  (28, 48446)\t0.42651891231156536\n",
      "  (28, 12948)\t0.48656471488380093\n",
      "  (39, 20619)\t0.40899278316264215\n",
      "  (39, 18209)\t0.34365993182262305\n",
      "  (39, 45512)\t0.3975122824640948\n",
      "  (39, 14911)\t0.7460608151288552\n",
      "  (49, 32490)\t0.42743786614899953\n",
      "  (49, 18034)\t0.2849164052110756\n",
      "  (49, 4670)\t0.46282346059154206\n",
      "  (49, 33477)\t0.2537370060095376\n",
      "  (49, 46295)\t0.28138808372957713\n",
      "  (49, 44051)\t0.3827862173857738\n",
      "  (49, 41829)\t0.42247277200644034\n",
      "  (49, 33250)\t0.23096255921394548\n",
      "  (112, 48190)\t0.40720794895959234\n",
      "  (112, 48871)\t0.3881546311106826\n",
      "  (112, 48746)\t0.35695158269974836\n",
      "  (112, 4709)\t0.42707814971563535\n",
      "  (112, 11356)\t0.33935255121190244\n",
      "  (112, 38836)\t0.5084755021444388\n",
      "  (135, 25982)\t0.24369015746316824\n",
      "  :\t:\n",
      "  (1279955, 49080)\t0.2555360950718457\n",
      "  (1279955, 51986)\t0.28701569924242565\n",
      "  (1279955, 9789)\t0.2710722058388166\n",
      "  (1279955, 20840)\t0.25141215463147515\n",
      "  (1279955, 21152)\t0.2252542333264861\n",
      "  (1279955, 39206)\t0.2010411555512863\n",
      "  (1279957, 19738)\t0.42755109468198393\n",
      "  (1279957, 51660)\t0.42755109468198393\n",
      "  (1279957, 22677)\t0.42755109468198393\n",
      "  (1279957, 19300)\t0.42755109468198393\n",
      "  (1279957, 36500)\t0.31175138264699687\n",
      "  (1279957, 16657)\t0.2678884099409027\n",
      "  (1279957, 16923)\t0.31598595060851903\n",
      "  (1279983, 30843)\t0.39643544131359926\n",
      "  (1279983, 7210)\t0.29638639690992413\n",
      "  (1279983, 21728)\t0.2665576405578542\n",
      "  (1279983, 17176)\t0.3631233631135948\n",
      "  (1279983, 51840)\t0.29548010864564106\n",
      "  (1279983, 15794)\t0.25699621461414285\n",
      "  (1279983, 33756)\t0.2728222979938918\n",
      "  (1279983, 53173)\t0.23523674133119493\n",
      "  (1279983, 26777)\t0.27264404544126036\n",
      "  (1279983, 20619)\t0.27096869274856666\n",
      "  (1279983, 18209)\t0.227683925755254\n",
      "  (1279983, 45512)\t0.26336255299634687\n",
      "  (1, 49841)\t0.3909193178488966\n",
      "  (1, 39072)\t0.6704903870230421\n",
      "  (1, 9805)\t0.42728033113445246\n",
      "  (1, 2378)\t0.4637415729346217\n",
      "  (8, 36645)\t0.5272106579837205\n",
      "  (8, 34109)\t0.44145176370720157\n",
      "  (8, 33250)\t0.27642809587009426\n",
      "  (8, 27870)\t0.3298551208879508\n",
      "  (8, 20799)\t0.36905438999160595\n",
      "  (8, 17335)\t0.4535980893850048\n",
      "  (24, 44420)\t0.3105761365998742\n",
      "  (24, 41672)\t0.41362012103042556\n",
      "  (24, 37681)\t0.4201739994852689\n",
      "  (24, 30836)\t0.29255332243343524\n",
      "  (24, 15385)\t0.3018519233590104\n",
      "  (24, 8350)\t0.3428640543591042\n",
      "  (24, 3643)\t0.24385430129637764\n",
      "  (24, 3536)\t0.28663519552977906\n",
      "  (24, 660)\t0.3464566690929015\n",
      "  (52, 51928)\t0.3914475852034\n",
      "  (52, 42992)\t0.591946238911667\n",
      "  (52, 8055)\t0.70453419950827\n",
      "  (81, 53792)\t0.2879217959801176\n",
      "  (81, 53173)\t0.17594752158940122\n",
      "  (81, 37217)\t0.2382114684962739\n",
      "  :\t:\n",
      "  (319966, 50836)\t0.28288631997032976\n",
      "  (319966, 47583)\t0.27413216616903263\n",
      "  (319966, 46841)\t0.278683593420915\n",
      "  (319966, 42227)\t0.3328926185126189\n",
      "  (319966, 41613)\t0.2664476862284724\n",
      "  (319966, 31151)\t0.21958746287569433\n",
      "  (319966, 28992)\t0.39177726243979016\n",
      "  (319966, 28374)\t0.24244956897549874\n",
      "  (319966, 102)\t0.4569821229766646\n",
      "  (319971, 48446)\t0.21598623688444232\n",
      "  (319971, 41352)\t0.3839970634266595\n",
      "  (319971, 34725)\t0.2274037514660351\n",
      "  (319971, 33756)\t0.22640371498849754\n",
      "  (319971, 31151)\t0.20739635368487938\n",
      "  (319971, 28649)\t0.37297245838927934\n",
      "  (319971, 26777)\t0.22625579071537658\n",
      "  (319971, 18326)\t0.21823100838161824\n",
      "  (319971, 16939)\t0.24933385006292197\n",
      "  (319971, 10981)\t0.3054029463530262\n",
      "  (319971, 6784)\t0.33901502241682524\n",
      "  (319971, 4654)\t0.27167791290594023\n",
      "  (319971, 2660)\t0.27894180775036\n",
      "  (319990, 53173)\t0.4135825046762163\n",
      "  (319990, 22505)\t0.6586788870132559\n",
      "  (319990, 15386)\t0.628563151663161\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "vectorizer=TfidfVectorizer()\n",
    "x_train=vectorizer.fit_transform(x_train)\n",
    "x_test=vectorizer.transform(x_test)\n",
    "print(x_train.shape,x_test.shape)\n",
    "print(x_train)\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eeb045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use logistic regression\n",
    "model=LogisticRegression(max_iter=100000)\n",
    "model.fit(x_train,y_train)\n",
    "x_train_pred=model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fedbaa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5626875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.12      0.22    159494\n",
      "           4       0.53      1.00      0.70    160506\n",
      "\n",
      "    accuracy                           0.56    320000\n",
      "   macro avg       0.77      0.56      0.46    320000\n",
      "weighted avg       0.77      0.56      0.46    320000\n",
      "\n",
      "[[ 19554 139940]\n",
      " [     0 160506]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cabb37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vectorizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('sentiment_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print(\"Model and vectorizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b57ff99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vectorizer saved!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "# Example: Sample data - Replace with your real data loading\n",
    "texts = [\n",
    "    \"I love this product\",\n",
    "    \"This is the worst experience ever\",\n",
    "    \"Not bad, could be better\",\n",
    "    \"Excellent customer service and fast delivery\"\n",
    "]\n",
    "labels = [1, 0, 1, 1]  # 1: Positive, 0: Negative\n",
    "\n",
    "# Create and fit the vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Train classifier\n",
    "model = LogisticRegression()\n",
    "model.fit(X, labels)\n",
    "\n",
    "# Save both model and vectorizer\n",
    "with open('sentiment_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "with open('vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "print(\"Model and vectorizer saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11c459fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1\n"
     ]
    }
   ],
   "source": [
    "# Load model and vectorizer\n",
    "with open('sentiment_model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "with open('vectorizer.pkl', 'rb') as file:\n",
    "    vectorizer = pickle.load(file)\n",
    "\n",
    "# Simulate input text\n",
    "text = \"This product is great!\"\n",
    "\n",
    "# Your preprocessing function here (match training preprocessing)\n",
    "processed_text = text.lower()\n",
    "\n",
    "# Vectorize\n",
    "vectorized_text = vectorizer.transform([processed_text])\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(vectorized_text)[0]\n",
    "print(f\"Prediction: {prediction}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
